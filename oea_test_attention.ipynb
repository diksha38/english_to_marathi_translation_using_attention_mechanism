{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7u5vAz7WOx5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model,load_model, model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import time\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/mar.txt','r') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "85fpOtFbWUB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncleaned_data_list = data.split('\\n')\n",
        "len(uncleaned_data_list)\n",
        "uncleaned_data_list = uncleaned_data_list[:38695]\n",
        "len(uncleaned_data_list)\n",
        "english_word = []\n",
        "marathi_word = []\n",
        "cleaned_data_list = []\n",
        "for word in uncleaned_data_list:\n",
        "  english_word.append(word.split('\\t')[:-1][0])\n",
        "  marathi_word.append(word.split('\\t')[:-1][1])\n",
        "language_data = pd.DataFrame(columns=['English','Marathi'])\n",
        "language_data['English'] = english_word\n",
        "language_data['Marathi'] = marathi_word\n",
        "language_data.to_csv('language_data.csv', index=False)"
      ],
      "metadata": {
        "id": "cFDHmVCsWUEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_data.shape\n",
        "language_data.head()\n",
        "language_data['English'].values\n",
        "language_data['Marathi'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbseLqZHWUGv",
        "outputId": "0d0fd3c9-9d29-42a2-fa0d-e850023f96f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['जा.', 'पळ!', 'धाव!', ..., 'मला ऑस्ट्रेलियात राहायचं नाहीये.',\n",
              "       'मला स्वतःबद्दल बोलायचं नाहीये.',\n",
              "       'मी चुकून माझी हार्ड डिस्क पुसून टाकली.'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = language_data['English'].values\n",
        "marathi_text = language_data['Marathi'].values\n",
        "len(english_text), len(marathi_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAM_E2qlWUI-",
        "outputId": "a80b3104-14b5-4b34-ff2c-f944379d8725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38695, 38695)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to lower case\n",
        "english_text_ = [x.lower() for x in english_text]\n",
        "marathi_text_ = [x.lower() for x in marathi_text]\n",
        "\n",
        "english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\n",
        "marathi_text_ = [re.sub(\"'\",'',x) for x in marathi_text_]\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation = string.punctuation\n",
        "    cleaned_text = ''.join(char for char in text if char not in punctuation)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_punctuation_from_list(text_list):\n",
        "    return [remove_punctuation(text) for text in text_list]\n",
        "\n",
        "# Applying the function to English and Marathi text\n",
        "english_text_ = remove_punctuation_from_list(english_text_)\n",
        "marathi_text_ = remove_punctuation_from_list(marathi_text_)\n",
        "\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "removed_digits_text = []\n",
        "for sent in english_text_:\n",
        "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
        "  removed_digits_text.append(' '.join(sentance))\n",
        "english_text_ = removed_digits_text\n",
        "\n",
        "# removing the digits from the marathi sentances\n",
        "marathi_text_ = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in marathi_text_]\n",
        "marathi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in marathi_text_]\n",
        "\n",
        "# removing the starting and ending whitespaces\n",
        "english_text_ = [x.strip() for x in english_text_]\n",
        "marathi_text_ = [x.strip() for x in marathi_text_]\n",
        "\n",
        "# Putting the start and end words in the marathi sentances\n",
        "marathi_text_ = [\"start \" + x + \" end\" for x in marathi_text_]\n",
        "# manipulated_marathi_text_\n",
        "marathi_text_[0], english_text_[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fJVs3bWULV",
        "outputId": "f4b4b3de-3115-425f-828d-7d4e44ae5998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('start जा end', 'go')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sent(text):\n",
        "  '''\n",
        "  Take list on texts as input and\n",
        "  returns its tokenizer and enocoded text\n",
        "  '''\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  return tokenizer, tokenizer.texts_to_sequences(text)\n",
        "  # Tokenize english and marathi sentences\n",
        "eng_tokenizer, eng_encoded= tokenize_sent(text= english_text_)\n",
        "mar_tokenizer, mar_encoded= tokenize_sent(text= marathi_text_)\n",
        "\n",
        "# English Word --> index dictionary\n",
        "eng_index_word = eng_tokenizer.index_word\n",
        "\n",
        "# English Index --> word dictionary\n",
        "eng_word_index= eng_tokenizer.word_index\n",
        "\n",
        "# size of English vocabulary for encoder input\n",
        "# For zero padding we have to add +1 in size\n",
        "ENG_VOCAB_SIZE = len(eng_tokenizer.word_counts)+1 #4494\n",
        "\n",
        "# Marathi Word --> index dict\n",
        "mar_word_index= mar_tokenizer.word_index\n",
        "\n",
        "# Marathi Index --> word dict\n",
        "mar_index_word = mar_tokenizer.index_word\n",
        "# marathi vocab size for decoder output\n",
        "MAR_VOCAB_SIZE=len(mar_tokenizer.word_counts)+1 #10642\n",
        "\n",
        "# Getting max length of English and Marathi sentences\n",
        "max_eng_len = 0\n",
        "for i in range(len(eng_encoded)):\n",
        "  if len(eng_encoded[i]) > max_eng_len:\n",
        "    max_eng_len= len(eng_encoded[i]) #9\n",
        "\n",
        "max_mar_len = 0\n",
        "for i in range(len(mar_encoded)):\n",
        "  if len(eng_encoded[i]) > max_mar_len:\n",
        "    max_mar_len= len(mar_encoded[i]) #9\n",
        "\n",
        "\n",
        "# Padding both\n",
        "eng_padded = pad_sequences(eng_encoded, maxlen=max_eng_len, padding='post')\n",
        "mar_padded = pad_sequences(mar_encoded, maxlen=max_mar_len, padding='post')\n",
        "\n",
        "# Convert to array\n",
        "eng_padded= np.array(eng_padded)\n",
        "mar_padded= np.array(mar_padded)\n",
        "\n",
        "# Split data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(eng_padded, mar_padded, test_size=0.1, random_state=0)\n"
      ],
      "metadata": {
        "id": "5gxaWA1LWUNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 25\n",
        "# BUFFER_SIZE stores the number of training points\n",
        "BUFFER_SIZE = len(X_train)\n",
        "\n",
        "# BATCH_SIZE is set to 64. Training and gradient descent happens in batches of 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# the number of batches in one epoch (also, the number of steps during training, when we go batch by batch)\n",
        "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
        "\n",
        "# the length of the embedded vector\n",
        "embedding_dim = 256\n",
        "\n",
        "# no of GRUs\n",
        "units = 1024\n",
        "\n",
        "# Hidden dimension\n",
        "hidden_dim = 1024\n",
        "\n",
        "# now, we shuffle the dataset and split it into batches of 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # the remainder after splitting by 64 are dropped\n",
        "\n",
        "print(BUFFER_SIZE)\n",
        "print(BUFFER_SIZE//64)\n",
        "print(steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv35RJKkWUPd",
        "outputId": "3b316986-7a74-4284-9148-4449682bab4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34825\n",
            "544\n",
            "544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "    def __init__(self, rnn_size, attention_func):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.attention_func = attention_func\n",
        "\n",
        "        if attention_func not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(\n",
        "                'Attention score must be either dot, general or concat.')\n",
        "\n",
        "        if attention_func == 'general':\n",
        "            # General score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "        elif attention_func == 'concat':\n",
        "            # Concat score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "            self.va = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, decoder_output, encoder_output):\n",
        "        if self.attention_func == 'dot':\n",
        "            # Dot score function: decoder_output (dot) encoder_output\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n",
        "        elif self.attention_func == 'general':\n",
        "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, self.wa(\n",
        "                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n",
        "        elif self.attention_func == 'concat':\n",
        "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
        "            # Decoder output must be broadcasted to encoder output's shape first\n",
        "            decoder_output = tf.tile(\n",
        "                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n",
        "\n",
        "            # Concat => Wa => va\n",
        "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
        "            score = self.va(\n",
        "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n",
        "\n",
        "            # Transpose score vector to have the same shape as other two above\n",
        "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
        "            score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n",
        "\n",
        "        # alignment a_t = softmax(score)\n",
        "        alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n",
        "\n",
        "        # context vector c_t is the weighted average sum of encoder output\n",
        "        context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n",
        "\n",
        "        return context, alignment"
      ],
      "metadata": {
        "id": "hVTIhQjzWUhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Define the embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define the RNN layer, LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        # Embed the input\n",
        "        embed = self.embedding(input_sequence)\n",
        "        # Call the LSTM unit\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        # Return a all 0s initial states\n",
        "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
        "                tf.zeros([batch_size, self.hidden_dim]))"
      ],
      "metadata": {
        "id": "v8wNSFFaYXFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, attention_func):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention = LuongAttention(hidden_dim, attention_func)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "        self.wc = tf.keras.layers.Dense(hidden_dim, activation='tanh')\n",
        "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input_sequence, state, encoder_output):\n",
        "        # Remember that the input to the decoder\n",
        "        # is now a batch of one-word sequences,\n",
        "        # which means that its shape is (batch_size, 1)\n",
        "        embed = self.embedding(input_sequence)\n",
        "\n",
        "        # Therefore, the lstm_out has shape (batch_size, 1, hidden_dim)\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
        "\n",
        "        # Use self.attention to compute the context and alignment vectors\n",
        "        # context vector's shape: (batch_size, 1, hidden_dim)\n",
        "        # alignment vector's shape: (batch_size, 1, source_length)\n",
        "        context, alignment = self.attention(lstm_out, encoder_output)\n",
        "\n",
        "        # Combine the context vector and the LSTM output\n",
        "        # Before combined, both have shape of (batch_size, 1, hidden_dim),\n",
        "        # so let's squeeze the axis 1 first\n",
        "        # After combined, it will have shape of (batch_size, 2 * hidden_dim)\n",
        "        lstm_out = tf.concat(\n",
        "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "\n",
        "        # lstm_out now has shape (batch_size, hidden_dim)\n",
        "        lstm_out = self.wc(lstm_out)\n",
        "\n",
        "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
        "        logits = self.ws(lstm_out)\n",
        "\n",
        "        return logits, state_h, state_c, alignment"
      ],
      "metadata": {
        "id": "3URMYLffYSn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(ENG_VOCAB_SIZE, embedding_dim, hidden_dim)"
      ],
      "metadata": {
        "id": "eaaeuWVfdYq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(MAR_VOCAB_SIZE, embedding_dim, hidden_dim, attention_func='concat')"
      ],
      "metadata": {
        "id": "oZ0lYAeje03E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def loss_func(targets, logits):\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    # Mask padding values, they do not have to compute for loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    # Calculate the loss value\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    # y_pred shape is batch_size, seq length, vocab size\n",
        "    # y_true shape is batch_size, seq length\n",
        "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
        "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "    # 0 is padding, don't include those\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * correct)\n",
        "    n_total = K.sum(mask)\n",
        "\n",
        "    return n_correct / n_total"
      ],
      "metadata": {
        "id": "FNTI5C-KY-Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "    ''' A training step, train a batch of the data and return the loss value reached\n",
        "        Input:\n",
        "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence\n",
        "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the target seq, our target sequence\n",
        "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence to the decoder, we use Teacher Forcing\n",
        "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
        "            the initial state of the encoder\n",
        "        - optimizer: a tf.keras.optimizers.\n",
        "        Output:\n",
        "        - loss: loss value\n",
        "\n",
        "    '''\n",
        "    loss = 0.\n",
        "    acc = 0.\n",
        "    logits = None\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "        en_states = en_outputs[1:]\n",
        "        de_state_h, de_state_c = en_states\n",
        "\n",
        "        # We need to create a loop to iterate through the target sequences\n",
        "        for i in range(target_seq_out.shape[1]):\n",
        "            # Input to the decoder must have shape of (batch_size, length)\n",
        "            # so we need to expand one dimension\n",
        "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "            logit, de_state_h, de_state_c, _ = decoder(\n",
        "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "\n",
        "            # The loss is now accumulated through the whole batch\n",
        "            loss += loss_func(target_seq_out[:, i], logit)\n",
        "            # Store the logits to calculate the accuracy\n",
        "            logit = K.expand_dims(logit, axis=1)\n",
        "            if logits is None:\n",
        "                logits = logit\n",
        "            else:\n",
        "                logits = K.concatenate((logits,logit), axis=1)\n",
        "        # Calculate the accuracy for the batch data\n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "    # Update the parameters and the optimizer\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss / target_seq_out.shape[1], acc"
      ],
      "metadata": {
        "id": "_tssXIvYYVdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
        "    for epoch in range(n_epochs):\n",
        "        start = time.time()\n",
        "        en_initial_states = encoder.init_states(batch_size)\n",
        "        for batch, (input_seq, target_seq) in enumerate(dataset.take(-1)):\n",
        "            target_seq_in = target_seq[:, :-1]\n",
        "            target_seq_out = target_seq[:, 1:]\n",
        "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, loss.numpy(), accuracy.numpy()))\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "        print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, loss.numpy(), accuracy.numpy()))\n",
        "        print('Time taken for 1 epoch {:.2f} sec\\n'.format(time.time() - start))\n",
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "# Train the model\n",
        "main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKrm9L2jf0Ij",
        "outputId": "1f2f4ad1-de13-4628-f085-fadfd7c648a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 6.1031 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 3.3490 Accuracy 0.2303\n",
            "Epoch 1 Batch 200 Loss 3.0762 Accuracy 0.2708\n",
            "Epoch 1 Batch 300 Loss 3.1131 Accuracy 0.3065\n",
            "Epoch 1 Batch 400 Loss 2.7275 Accuracy 0.3396\n",
            "Epoch 1 Batch 500 Loss 2.6202 Accuracy 0.3658\n",
            "Epoch 1 Loss 2.5731 Accuracy 0.3780\n",
            "Time taken for 1 epoch 71.05 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.3915 Accuracy 0.3940\n",
            "Epoch 2 Batch 100 Loss 2.1228 Accuracy 0.4465\n",
            "Epoch 2 Batch 200 Loss 1.9210 Accuracy 0.4247\n",
            "Epoch 2 Batch 300 Loss 1.8332 Accuracy 0.5268\n",
            "Epoch 2 Batch 400 Loss 1.6776 Accuracy 0.5539\n",
            "Epoch 2 Batch 500 Loss 1.3972 Accuracy 0.6000\n",
            "Epoch 2 Loss 1.4120 Accuracy 0.5868\n",
            "Time taken for 1 epoch 47.86 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2992 Accuracy 0.5920\n",
            "Epoch 3 Batch 100 Loss 1.0727 Accuracy 0.6706\n",
            "Epoch 3 Batch 200 Loss 1.1458 Accuracy 0.6626\n",
            "Epoch 3 Batch 300 Loss 1.0825 Accuracy 0.6486\n",
            "Epoch 3 Batch 400 Loss 0.8656 Accuracy 0.7273\n",
            "Epoch 3 Batch 500 Loss 0.9004 Accuracy 0.7098\n",
            "Epoch 3 Loss 0.9542 Accuracy 0.6754\n",
            "Time taken for 1 epoch 41.55 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7267 Accuracy 0.7151\n",
            "Epoch 4 Batch 100 Loss 0.6282 Accuracy 0.7620\n",
            "Epoch 4 Batch 200 Loss 0.7442 Accuracy 0.7169\n",
            "Epoch 4 Batch 300 Loss 0.8421 Accuracy 0.7333\n",
            "Epoch 4 Batch 400 Loss 0.7118 Accuracy 0.7117\n",
            "Epoch 4 Batch 500 Loss 0.6344 Accuracy 0.7426\n",
            "Epoch 4 Loss 0.5710 Accuracy 0.7538\n",
            "Time taken for 1 epoch 43.57 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4076 Accuracy 0.8367\n",
            "Epoch 5 Batch 100 Loss 0.4905 Accuracy 0.8047\n",
            "Epoch 5 Batch 200 Loss 0.5478 Accuracy 0.7853\n",
            "Epoch 5 Batch 300 Loss 0.5424 Accuracy 0.7722\n",
            "Epoch 5 Batch 400 Loss 0.5292 Accuracy 0.7788\n",
            "Epoch 5 Batch 500 Loss 0.5014 Accuracy 0.8165\n",
            "Epoch 5 Loss 0.6277 Accuracy 0.7470\n",
            "Time taken for 1 epoch 81.91 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2991 Accuracy 0.8606\n",
            "Epoch 6 Batch 100 Loss 0.3543 Accuracy 0.8333\n",
            "Epoch 6 Batch 200 Loss 0.3996 Accuracy 0.8171\n",
            "Epoch 6 Batch 300 Loss 0.3988 Accuracy 0.8462\n",
            "Epoch 6 Batch 400 Loss 0.4226 Accuracy 0.8228\n",
            "Epoch 6 Batch 500 Loss 0.4326 Accuracy 0.8055\n",
            "Epoch 6 Loss 0.5288 Accuracy 0.7855\n",
            "Time taken for 1 epoch 83.02 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2574 Accuracy 0.8408\n",
            "Epoch 7 Batch 100 Loss 0.2226 Accuracy 0.8908\n",
            "Epoch 7 Batch 200 Loss 0.3358 Accuracy 0.8588\n",
            "Epoch 7 Batch 300 Loss 0.3239 Accuracy 0.8464\n",
            "Epoch 7 Batch 400 Loss 0.3613 Accuracy 0.8329\n",
            "Epoch 7 Batch 500 Loss 0.4531 Accuracy 0.8313\n",
            "Epoch 7 Loss 0.3055 Accuracy 0.8450\n",
            "Time taken for 1 epoch 41.65 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2502 Accuracy 0.8991\n",
            "Epoch 8 Batch 100 Loss 0.3100 Accuracy 0.8559\n",
            "Epoch 8 Batch 200 Loss 0.1915 Accuracy 0.9096\n",
            "Epoch 8 Batch 300 Loss 0.2469 Accuracy 0.8938\n",
            "Epoch 8 Batch 400 Loss 0.4217 Accuracy 0.8282\n",
            "Epoch 8 Batch 500 Loss 0.2918 Accuracy 0.8515\n",
            "Epoch 8 Loss 0.3650 Accuracy 0.8410\n",
            "Time taken for 1 epoch 47.19 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2587 Accuracy 0.8671\n",
            "Epoch 9 Batch 100 Loss 0.2734 Accuracy 0.8647\n",
            "Epoch 9 Batch 200 Loss 0.2620 Accuracy 0.8669\n",
            "Epoch 9 Batch 300 Loss 0.2615 Accuracy 0.9021\n",
            "Epoch 9 Batch 400 Loss 0.3058 Accuracy 0.8575\n",
            "Epoch 9 Batch 500 Loss 0.3512 Accuracy 0.8287\n",
            "Epoch 9 Loss 0.2918 Accuracy 0.8478\n",
            "Time taken for 1 epoch 41.27 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2601 Accuracy 0.8731\n",
            "Epoch 10 Batch 100 Loss 0.2389 Accuracy 0.8758\n",
            "Epoch 10 Batch 200 Loss 0.2418 Accuracy 0.8720\n",
            "Epoch 10 Batch 300 Loss 0.2631 Accuracy 0.8898\n",
            "Epoch 10 Batch 400 Loss 0.2303 Accuracy 0.8642\n",
            "Epoch 10 Batch 500 Loss 0.2565 Accuracy 0.8677\n",
            "Epoch 10 Loss 0.3244 Accuracy 0.8614\n",
            "Time taken for 1 epoch 47.08 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2211 Accuracy 0.8792\n",
            "Epoch 11 Batch 100 Loss 0.1651 Accuracy 0.9204\n",
            "Epoch 11 Batch 200 Loss 0.3198 Accuracy 0.8673\n",
            "Epoch 11 Batch 300 Loss 0.2112 Accuracy 0.8847\n",
            "Epoch 11 Batch 400 Loss 0.2703 Accuracy 0.8538\n",
            "Epoch 11 Batch 500 Loss 0.2774 Accuracy 0.8706\n",
            "Epoch 11 Loss 0.2987 Accuracy 0.8617\n",
            "Time taken for 1 epoch 41.25 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2519 Accuracy 0.8727\n",
            "Epoch 12 Batch 100 Loss 0.1747 Accuracy 0.8839\n",
            "Epoch 12 Batch 200 Loss 0.2490 Accuracy 0.8758\n",
            "Epoch 12 Batch 300 Loss 0.3056 Accuracy 0.8626\n",
            "Epoch 12 Batch 400 Loss 0.2157 Accuracy 0.8985\n",
            "Epoch 12 Batch 500 Loss 0.3033 Accuracy 0.8628\n",
            "Epoch 12 Loss 0.2499 Accuracy 0.8761\n",
            "Time taken for 1 epoch 47.40 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2048 Accuracy 0.8960\n",
            "Epoch 13 Batch 100 Loss 0.1821 Accuracy 0.9061\n",
            "Epoch 13 Batch 200 Loss 0.1872 Accuracy 0.9226\n",
            "Epoch 13 Batch 300 Loss 0.2621 Accuracy 0.8822\n",
            "Epoch 13 Batch 400 Loss 0.2322 Accuracy 0.8818\n",
            "Epoch 13 Batch 500 Loss 0.2432 Accuracy 0.8721\n",
            "Epoch 13 Loss 0.2725 Accuracy 0.8932\n",
            "Time taken for 1 epoch 41.20 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2434 Accuracy 0.8935\n",
            "Epoch 14 Batch 100 Loss 0.2196 Accuracy 0.8757\n",
            "Epoch 14 Batch 200 Loss 0.2089 Accuracy 0.8935\n",
            "Epoch 14 Batch 300 Loss 0.2644 Accuracy 0.8792\n",
            "Epoch 14 Batch 400 Loss 0.2418 Accuracy 0.8860\n",
            "Epoch 14 Batch 500 Loss 0.2659 Accuracy 0.8698\n",
            "Epoch 14 Loss 0.2972 Accuracy 0.8697\n",
            "Time taken for 1 epoch 47.21 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1644 Accuracy 0.9118\n",
            "Epoch 15 Batch 100 Loss 0.2137 Accuracy 0.8851\n",
            "Epoch 15 Batch 200 Loss 0.1905 Accuracy 0.9150\n",
            "Epoch 15 Batch 300 Loss 0.2611 Accuracy 0.8860\n",
            "Epoch 15 Batch 400 Loss 0.2497 Accuracy 0.8924\n",
            "Epoch 15 Batch 500 Loss 0.2430 Accuracy 0.8921\n",
            "Epoch 15 Loss 0.2424 Accuracy 0.8879\n",
            "Time taken for 1 epoch 41.25 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1374 Accuracy 0.9240\n",
            "Epoch 16 Batch 100 Loss 0.1841 Accuracy 0.9083\n",
            "Epoch 16 Batch 200 Loss 0.1572 Accuracy 0.9009\n",
            "Epoch 16 Batch 300 Loss 0.1900 Accuracy 0.8991\n",
            "Epoch 16 Batch 400 Loss 0.1854 Accuracy 0.9027\n",
            "Epoch 16 Batch 500 Loss 0.2541 Accuracy 0.8883\n",
            "Epoch 16 Loss 0.2943 Accuracy 0.8795\n",
            "Time taken for 1 epoch 47.25 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1667 Accuracy 0.9201\n",
            "Epoch 17 Batch 100 Loss 0.1499 Accuracy 0.9187\n",
            "Epoch 17 Batch 200 Loss 0.2046 Accuracy 0.9043\n",
            "Epoch 17 Batch 300 Loss 0.1298 Accuracy 0.9176\n",
            "Epoch 17 Batch 400 Loss 0.1653 Accuracy 0.8915\n",
            "Epoch 17 Batch 500 Loss 0.2431 Accuracy 0.8792\n",
            "Epoch 17 Loss 0.2383 Accuracy 0.8687\n",
            "Time taken for 1 epoch 41.37 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2074 Accuracy 0.8932\n",
            "Epoch 18 Batch 100 Loss 0.1824 Accuracy 0.8944\n",
            "Epoch 18 Batch 200 Loss 0.2112 Accuracy 0.8864\n",
            "Epoch 18 Batch 300 Loss 0.1966 Accuracy 0.8902\n",
            "Epoch 18 Batch 400 Loss 0.1495 Accuracy 0.9145\n",
            "Epoch 18 Batch 500 Loss 0.2445 Accuracy 0.8675\n",
            "Epoch 18 Loss 0.2307 Accuracy 0.8972\n",
            "Time taken for 1 epoch 44.93 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2009 Accuracy 0.8855\n",
            "Epoch 19 Batch 100 Loss 0.1818 Accuracy 0.9134\n",
            "Epoch 19 Batch 200 Loss 0.1827 Accuracy 0.9049\n",
            "Epoch 19 Batch 300 Loss 0.2778 Accuracy 0.8496\n",
            "Epoch 19 Batch 400 Loss 0.2035 Accuracy 0.8930\n",
            "Epoch 19 Batch 500 Loss 0.1861 Accuracy 0.9088\n",
            "Epoch 19 Loss 0.1732 Accuracy 0.8930\n",
            "Time taken for 1 epoch 41.08 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1300 Accuracy 0.9341\n",
            "Epoch 20 Batch 100 Loss 0.1313 Accuracy 0.9226\n",
            "Epoch 20 Batch 200 Loss 0.1864 Accuracy 0.9050\n",
            "Epoch 20 Batch 300 Loss 0.1974 Accuracy 0.9110\n",
            "Epoch 20 Batch 400 Loss 0.2232 Accuracy 0.8964\n",
            "Epoch 20 Batch 500 Loss 0.1674 Accuracy 0.9169\n",
            "Epoch 20 Loss 0.2188 Accuracy 0.8991\n",
            "Time taken for 1 epoch 47.11 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.1398 Accuracy 0.9186\n",
            "Epoch 21 Batch 100 Loss 0.1722 Accuracy 0.9150\n",
            "Epoch 21 Batch 200 Loss 0.1812 Accuracy 0.8860\n",
            "Epoch 21 Batch 300 Loss 0.1880 Accuracy 0.9065\n",
            "Epoch 21 Batch 400 Loss 0.2342 Accuracy 0.8866\n",
            "Epoch 21 Batch 500 Loss 0.1853 Accuracy 0.8772\n",
            "Epoch 21 Loss 0.2241 Accuracy 0.8724\n",
            "Time taken for 1 epoch 41.21 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.1351 Accuracy 0.9357\n",
            "Epoch 22 Batch 100 Loss 0.1553 Accuracy 0.9177\n",
            "Epoch 22 Batch 200 Loss 0.1214 Accuracy 0.9284\n",
            "Epoch 22 Batch 300 Loss 0.1746 Accuracy 0.9042\n",
            "Epoch 22 Batch 400 Loss 0.2114 Accuracy 0.8866\n",
            "Epoch 22 Batch 500 Loss 0.1606 Accuracy 0.9038\n",
            "Epoch 22 Loss 0.1727 Accuracy 0.8879\n",
            "Time taken for 1 epoch 47.19 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.1731 Accuracy 0.8957\n",
            "Epoch 23 Batch 100 Loss 0.1158 Accuracy 0.9364\n",
            "Epoch 23 Batch 200 Loss 0.1386 Accuracy 0.9133\n",
            "Epoch 23 Batch 300 Loss 0.1949 Accuracy 0.8936\n",
            "Epoch 23 Batch 400 Loss 0.1829 Accuracy 0.9110\n",
            "Epoch 23 Batch 500 Loss 0.2203 Accuracy 0.8867\n",
            "Epoch 23 Loss 0.1959 Accuracy 0.8971\n",
            "Time taken for 1 epoch 41.18 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.1349 Accuracy 0.9121\n",
            "Epoch 24 Batch 100 Loss 0.1342 Accuracy 0.9077\n",
            "Epoch 24 Batch 200 Loss 0.1798 Accuracy 0.8677\n",
            "Epoch 24 Batch 300 Loss 0.1752 Accuracy 0.8879\n",
            "Epoch 24 Batch 400 Loss 0.1499 Accuracy 0.9056\n",
            "Epoch 24 Batch 500 Loss 0.1920 Accuracy 0.8878\n",
            "Epoch 24 Loss 0.2136 Accuracy 0.8772\n",
            "Time taken for 1 epoch 47.22 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.1583 Accuracy 0.8918\n",
            "Epoch 25 Batch 100 Loss 0.1417 Accuracy 0.9136\n",
            "Epoch 25 Batch 200 Loss 0.1297 Accuracy 0.9273\n",
            "Epoch 25 Batch 300 Loss 0.1651 Accuracy 0.8988\n",
            "Epoch 25 Batch 400 Loss 0.2150 Accuracy 0.8899\n",
            "Epoch 25 Batch 500 Loss 0.1556 Accuracy 0.9069\n",
            "Epoch 25 Loss 0.1808 Accuracy 0.8931\n",
            "Time taken for 1 epoch 41.17 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_seq2seq_att(input_sentence, encoder, max_input_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n",
        "    if input_sentence is None:\n",
        "        input_sentence = input_data[np.random.choice(len(input_data))]\n",
        "    print(\"Input Sentence:\", input_sentence)\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_sentence])\n",
        "    # Pad the sentence\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
        "    # Get the encoder initial states\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    # Get the encoder outputs or hidden states\n",
        "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
        "    # Set the decoder input to the sos token\n",
        "    de_input = tf.constant([[word2idx_outputs['start']]])\n",
        "    # Set the initial hidden states of the decoder to the hidden states of the encoder\n",
        "    de_state_h, de_state_c = en_outputs[1:]\n",
        "\n",
        "    out_words = []\n",
        "    alignments = []\n",
        "\n",
        "    while True:\n",
        "        # Get the decoder with attention output\n",
        "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
        "            de_input, (de_state_h, de_state_c), en_outputs[0])\n",
        "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "        # Detokenize the output\n",
        "        predicted_word = idx2word_outputs[de_input.numpy()[0][0]]\n",
        "        if predicted_word == 'end' or len(out_words) >= 20:\n",
        "            break\n",
        "        out_words.append(predicted_word)\n",
        "        # Save the alignment matrix\n",
        "        alignments.append(alignment.numpy())\n",
        "\n",
        "    # Join the output words\n",
        "    predicted_sequence = ' '.join(out_words)\n",
        "    print(\"Predicted Output Sequence:\", predicted_sequence)\n",
        "\n",
        "    return np.array(alignments), input_sentence.split(' '), out_words\n",
        "\n",
        "# Example usage:\n",
        "input_sentence = \"i do not like my dress\"\n",
        "# Predict the output sequence\n",
        "output_sequence = predict_seq2seq_att(input_sentence, encoder, max_eng_len, eng_tokenizer, mar_word_index, mar_index_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMFZouh1WUjv",
        "outputId": "fbca2359-25f6-4238-a085-4e5aa537c988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence: i do not like my dress\n",
            "Predicted Output Sequence: मला माझा ड्रेस आवडला नाही\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5t5yw_AJq7FU"
      }
    }
  ]
}